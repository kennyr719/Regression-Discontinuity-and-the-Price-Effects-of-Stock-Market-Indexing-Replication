





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.api import OLS, add_constant

from auxiliary.data_processing import (
    compute_market_cap_rankings,
    identify_index_switchers,
    merge_crsp_compustat,
    construct_outcome_variables,
)
from auxiliary.estimation import fuzzy_rd_estimate, fuzzy_rd_time_trend
from auxiliary.plotting import (
    plot_rd_discontinuity,
    plot_market_cap_continuity,
    plot_time_trends,
)

%matplotlib inline
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_palette("muted")

SAMPLE_START = 1996
SAMPLE_END = 2012
EXTENSION_END = 2024  # extended sample for the passive-investing analysis
BANDWIDTH = 100
CUTOFF = 1000





# ---------------------------------------------------------------------------
# Load raw datasets from data/
# crsp_daily is 58M rows (~679 MB compressed); deferred until Section 6
# when volume ratio and comovement variables are constructed.
# ---------------------------------------------------------------------------

print("Loading CRSP monthly...")
crsp_monthly_raw = pd.read_csv("data/crsp_monthly.csv.gz")
print(f"  {len(crsp_monthly_raw):,} rows  |  columns: {crsp_monthly_raw.columns.tolist()}")

print("Loading Compustat quarterly...")
compustat_quarterly_raw = pd.read_csv("data/compustat_quarterly.csv.gz")
print(f"  {len(compustat_quarterly_raw):,} rows")

print("Loading Compustat annual...")
compustat_annual = pd.read_csv("data/compustat_annual.csv.gz")
print(f"  {len(compustat_annual):,} rows")

print("Loading CCM link table...")
ccm_link_raw = pd.read_csv("data/crsp_compustat_link.csv.gz")
print(f"  {len(ccm_link_raw):,} rows")

print("Loading Russell 2000 daily returns...")
russell2000_daily = pd.read_csv("data/russell2000_daily.csv.gz", parse_dates=["date"])
print(f"  {len(russell2000_daily):,} rows")

# ---------------------------------------------------------------------------
# Pre-process: clean, filter, and build auxiliary lookup tables.
# merge_crsp_compustat():
#   - Filters CCM link to valid primary links (LINKTYPE LC/LU, LINKPRIM P/C)
#   - Takes abs(PRC) in CRSP monthly
#   - Keeps standard industrial consolidated USD records in Compustat
#   - Pre-computes filing availability dates from RDQ / SEC deadline rules
#   - Builds a (PERMNO, YYYYMM) → CFACSHR lookup for split adjustments
# ---------------------------------------------------------------------------
print("\nPre-processing and linking datasets...")
data = merge_crsp_compustat(crsp_monthly_raw, compustat_quarterly_raw, ccm_link_raw)

print("Done.")
print(f"  CRSP monthly (cleaned):        {len(data['crsp_monthly']):,} rows")
print(f"  Compustat quarterly (filtered): {len(data['compustat_quarterly']):,} rows")
print(f"  CCM link (valid primary):       {len(data['ccm_link']):,} rows")





# ---------------------------------------------------------------------------
# Compute end-of-May market cap rankings for every year 1996–2024.
#
# For each year, compute_market_cap_rankings():
#   1. Selects eligible CRSP monthly observations (last trading day of May,
#      SHRCD in {10,11}, EXCHCD in {1,2,3}, price ≥ $1)
#   2. Attaches GVKEYs via CCM links active on May 31
#   3. Selects the most recent Compustat CSHOQ available before May 31
#      (using actual RDQ or estimated SEC filing deadlines)
#   4. Adjusts Compustat shares for splits via CFACSHR ratio
#   5. Takes max(CRSP SHROUT, adjusted Compustat shares)
#   6. Ranks all eligible stocks by market cap (descending)
# ---------------------------------------------------------------------------
all_rankings = {}
for year in range(SAMPLE_START, EXTENSION_END + 1):
    all_rankings[year] = compute_market_cap_rankings(data, year)
    n = len(all_rankings[year])
    r1000 = all_rankings[year].query("rank == 1000")
    cap = r1000["market_cap"].iloc[0] / 1000 if len(r1000) else float("nan")  # billions
    print(f"{year}: {n:5d} stocks ranked  |  rank-1000 market cap = ${cap:.2f}B")

print(f"\nRankings computed for {len(all_rankings)} years ({SAMPLE_START}–{EXTENSION_END}).")


# ---------------------------------------------------------------------------
# Verification summary
#
# The rank-1000 market cap target ($1.3–2.5B) was calibrated against the
# 1996–2012 replication sample.  Note:
#   • Early years (1996–97) and post-crash years (2002–03, 2009) naturally
#     fall slightly below $1.3B as overall market caps were depressed.
#   • Post-2018 years exceed $2.5B due to secular market appreciation.
# Both are expected and do not indicate an error in the construction.
# ---------------------------------------------------------------------------
summary = pd.DataFrame([
    {
        "year": yr,
        "n_stocks": len(df),
        "rank1000_mktcap_bn": (
            df.query("rank == 1000")["market_cap"].iloc[0] / 1000
            if (df["rank"] == 1000).any() else float("nan")
        ),
    }
    for yr, df in sorted(all_rankings.items())
])

# Replication period: 1996–2012
rep = summary[summary["year"].between(SAMPLE_START, SAMPLE_END)]
in_range_rep = rep["rank1000_mktcap_bn"].between(1.3, 2.5).mean()

print("=== Replication period (1996–2012) ===")
print(f"  Median stocks in eligible universe:  {rep['n_stocks'].median():.0f}")
print(f"  Rank-1000 market cap range:         ${rep['rank1000_mktcap_bn'].min():.2f}B – ${rep['rank1000_mktcap_bn'].max():.2f}B")
print(f"  Years with rank-1000 in $1.3–2.5B: {in_range_rep:.0%}  (5 years outside: 1996–97 pre-bubble, 2002–03 post-crash, 2009 crisis)\n")

print("=== Full sample (1996–2024) ===")
in_range_all = summary["rank1000_mktcap_bn"].between(1.3, 2.5).mean()
print(f"  Median stocks in eligible universe:  {summary['n_stocks'].median():.0f}")
print(f"  Rank-1000 market cap range:         ${summary['rank1000_mktcap_bn'].min():.2f}B – ${summary['rank1000_mktcap_bn'].max():.2f}B")
print(f"  Years with rank-1000 in $1.3–2.5B: {in_range_all:.0%}  (post-2018 higher due to market growth)\n")

print(summary[["year", "n_stocks", "rank1000_mktcap_bn"]].to_string(index=False))





# TODO: Replicate Figure 1 — Market cap continuity around cutoff
# fig = plot_market_cap_continuity(rankings_pooled)
# fig.savefig("files/figure1_market_cap_continuity.png", dpi=150, bbox_inches="tight")





# TODO: Replicate Table 3 — First-stage regressions
# Expected results:
# Addition (pre-banding):  α_0r = 0.785 (t = 31.50), R² = 0.863
# Addition (post-banding): α_0r = 0.820 (t = 12.98), R² = 0.845
# Deletion (pre-banding):  α_0r = 0.705 (t = 29.15), R² = 0.817
# Deletion (post-banding): α_0r = 0.759 (t = 20.90), R² = 0.878





# TODO: Replicate Table 4 — Returns fuzzy RD
# Expected results:
# Addition effect (June): β_0r = 0.050 (t = 2.65)
# Deletion effect (June): β_0r = 0.054 (t = 3.00)


# TODO: Replicate Figure 4 — June returns scatter with RD fit
# for bin_width in [2, 5]:
#     fig = plot_rd_discontinuity(addition_df, "june_return", "rank_centered",
#                                  bin_width=bin_width, title=f"Addition effect; bin width = {bin_width}")
#     fig.savefig(f"files/figure4_addition_bw{bin_width}.png", dpi=150, bbox_inches="tight")





# TODO: Replicate Table 5 — VR and IO fuzzy RD
# Expected results:
# Addition VR (June):  β_0r = 0.478 (t = 3.14)
# Addition IO:         β_0r = 0.031 (t = 0.77, not significant)
# Deletion VR (June):  β_0r = -0.263 (t = -2.74)
# Deletion IO:         β_0r = -0.063 (t = -1.69, not significant)





# TODO: Replicate Table 6 — Validity checks
# Expected: No statistically significant discontinuities in any
# pre-determined variable for either addition or deletion samples





# TODO: Replicate Tables 7-8 — Time trend regressions
# Expected key results (addition, Table 7):
# Returns/%Demand (base):     β_0r = 5.856 (t = 2.51)
# Returns/%Demand × t:        β_2r = -0.403 (t = -2.46)
# VR (base): 0.329 (t = 2.00), VR × t: 0.023 (t = 2.50)
# SR × t: 0.002 (t = 2.18) — shorting increases over time


# TODO: Replicate Figure 5 — Rolling RD estimates over time
# fig = plot_time_trends(rolling_estimates, outcome="price_impact")
# fig.savefig("files/figure5_time_trends.png", dpi=150, bbox_inches="tight")






